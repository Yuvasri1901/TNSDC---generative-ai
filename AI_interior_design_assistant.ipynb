{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHCL9bydQVibSjON8TASbl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yuvasri1901/TNSDC---generative-ai/blob/main/AI_interior_design_assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "label the images"
      ],
      "metadata": {
        "id": "GwKzKFIkA4JV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "! mkdir dataset\n",
        "# Path to your downloaded dataset\n",
        "dataset_path = Path(\"/content/interior_design_dataset.zip\")\n",
        "! mkdir labeled_dataset\n",
        "\n",
        "# Create a dictionary to map folder names to labels\n",
        "label_map = {\n",
        "    \"Modern\": 0,\n",
        "    \"Vintage\": 1,\n",
        "    \"Minimalist\": 2,\n",
        "    # Add more labels as needed\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVLZ4KCc6v6v",
        "outputId": "342444ec-fb22-4a21-e630-c69d2e669686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘labeled_dataset’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# Path to your downloaded dataset\n",
        "dataset_path = Path(\"/content/interior_design_dataset.zip\")\n",
        "\n",
        "# Create a dictionary to map folder names to labels\n",
        "label_map = {\n",
        "    \"Modern\": 0,\n",
        "    \"Vintage\": 1,\n",
        "    \"Minimalist\": 2,\n",
        "    # Add more labels as needed\n",
        "}\n",
        "\n",
        "# Path to the directory where labeled images will be saved\n",
        "labeled_dataset_path = Path(\"/content/labeled_dataset\")\n",
        "\n",
        "# Extract the dataset\n",
        "with ZipFile(dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/dataset\")\n",
        "\n",
        "# Iterate through the dataset folders\n",
        "for folder_name in os.listdir(\"/content/dataset\"):\n",
        "    if folder_name in label_map:\n",
        "        label = label_map[folder_name]\n",
        "        folder_path = Path(\"/content/dataset\") / folder_name\n",
        "        # Iterate through images in the folder\n",
        "        for image_name in os.listdir(folder_path):\n",
        "            image_path = folder_path / image_name\n",
        "            # Create a new folder for the label if it doesn't exist\n",
        "            label_folder = labeled_dataset_path / str(label)\n",
        "            label_folder.mkdir(parents=True, exist_ok=True)\n",
        "            # Copy the image to the corresponding label folder\n",
        "            shutil.copy(image_path, label_folder)\n",
        "\n",
        "print(\"Images labeled and saved.\")\n"
      ],
      "metadata": {
        "id": "pnpmqAed5Ga6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new directory for labeled dataset\n",
        "labeled_dataset_path = Path(\"/content/labeled_dataset\")\n",
        "os.makedirs(labeled_dataset_path, exist_ok=True)\n",
        "\n",
        "# Iterate through each subfolder (class) in the dataset\n",
        "for folder_name in os.listdir(\"dataset\"):\n",
        "    if folder_name in label_map:\n",
        "        label = label_map[folder_name]\n",
        "        folder_path = \"/content/archive.zip\" / folder_name\n",
        "        labeled_folder_path = labeled_dataset_path / folder_name\n",
        "        os.makedirs(labeled_folder_path, exist_ok=True)\n",
        "\n",
        "        # Copy images from original folder to the labeled folder with updated names\n",
        "        for idx, image_name in enumerate(os.listdir(folder_path)):\n",
        "            old_image_path = folder_path / image_name\n",
        "            new_image_name = f\"{label}_{idx}.jpg\"  # Assign a new name with label\n",
        "            new_image_path = labeled_folder_path / new_image_name\n",
        "            shutil.copyfile(old_image_path, new_image_path)\n",
        "\n",
        "print(\"Dataset labeling completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uZR_81uDbuJ",
        "outputId": "085bdd48-ad92-496d-dba1-2bb0ee71f329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset labeling completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocess the labeled images"
      ],
      "metadata": {
        "id": "wHBmLtjmAtNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Path to the labeled dataset\n",
        "labeled_dataset_path = Path(\"/content/labeled_dataset\")\n",
        "\n",
        "# Define the ImageDataGenerator for preprocessing\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,  # Normalize pixel values to [0, 1]\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Define batch size and image size\n",
        "batch_size = 32\n",
        "image_size = (256, 256)\n",
        "\n",
        "# Create a generator for training data\n",
        "train_generator = datagen.flow_from_directory(labeled_dataset_path,target_size=image_size,batch_size=batch_size,class_mode='categorical',shuffle=True)\n",
        "\n",
        "print(len(train_generator))\n",
        "!ls {labeled_dataset_path}\n",
        "print(batch_size)\n",
        "# Visualize a few preprocessed images\n",
        "sample_images, sample_labels = next(train_generator)\n",
        "\n",
        "plt.figure(figsize=(12, 9))\n",
        "for i in range(9):\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(sample_images[i])\n",
        "    plt.title(f\"Label: {np.argmax(sample_labels[i])}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "aRhUK_Kz9sK4",
        "outputId": "0e2336e7-4c56-424f-f7ca-f180507069ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 images belonging to 0 classes.\n",
            "0\n",
            "32\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 0 is out of bounds for axis 0 with size 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-12da68dc2ae3>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Label: {np.argmax(sample_labels[i])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x900 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from zipfile import ZipFile\n",
        "import os\n",
        "\n",
        "# Path to the labeled zip dataset\n",
        "zip_dataset_path = \"/content/interior_design_dataset.zip\"\n",
        "extracted_path = \"/content/labeled_dataset\"\n",
        "\n",
        "# Extract the zip file\n",
        "with ZipFile(zip_dataset_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_path)\n",
        "\n",
        "# Now, define the path to the extracted dataset\n",
        "extracted_dataset_path = os.path.join(extracted_path, \"labeled_dataset\")\n",
        "\n",
        "# Define the ImageDataGenerator for preprocessing\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,  # Normalize pixel values to [0, 1]\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Define batch size and image size\n",
        "batch_size = 32\n",
        "image_size = (256, 256)\n",
        "\n",
        "# Create a generator for training data\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    extracted_dataset_path,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(len(train_generator))\n",
        "print(os.listdir(extracted_dataset_path))\n",
        "print(batch_size)\n",
        "\n",
        "# Visualize a few preprocessed images\n",
        "sample_images, sample_labels = next(train_generator)\n",
        "\n",
        "plt.figure(figsize=(12, 9))\n",
        "for i in range(9):\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(sample_images[i])\n",
        "    plt.title(f\"Label: {np.argmax(sample_labels[i])}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "NXuRueiP0lkt",
        "outputId": "a46ac6de-cde2-4f6b-d17b-fe66bfc00104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/labeled_dataset/labeled_dataset'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-017041a76f0f>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Create a generator for training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m train_generator = datagen.flow_from_directory(\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mextracted_dataset_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m         \"\"\"\n\u001b[0;32m-> 1649\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1650\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/labeled_dataset/labeled_dataset'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of images: {len(sample_images)}\")\n",
        "print(f\"Number of labels: {len(sample_labels)}\")\n",
        "if len(sample_images) == 0 or len(sample_labels) == 0:\n",
        "    print(\"Error: The sample_images and sample_labels variables are empty.\")\n",
        "print(train_generator)\n",
        "print(f\"Batch size: {train_generator.batch_size}\")\n",
        "print(f\"Target size: {train_generator.target_size}\")\n",
        "print(f\"Class mode: {train_generator.class_mode}\")\n",
        "print(f\"Shuffle: {train_generator.shuffle}\")\n",
        "print(labeled_dataset_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGQqnhuAzPpk",
        "outputId": "6346734c-a37b-4976-914b-4de36bfccd69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images: 0\n",
            "Number of labels: 0\n",
            "Error: The sample_images and sample_labels variables are empty.\n",
            "<keras.src.preprocessing.image.DirectoryIterator object at 0x78c18bf3f760>\n",
            "Batch size: 16\n",
            "Target size: (256, 256)\n",
            "Class mode: categorical\n",
            "Shuffle: True\n",
            "/content/labeled_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls {extracted_dataset_path}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxL-OAq41OD8",
        "outputId": "ce9a3872-d3a9-44e4-e759-4ff52123fdf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/labeled_dataset/labeled_dataset': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Path to the labeled dataset\n",
        "labeled_dataset_path = Path(\"/content/labeled_dataset\")\n",
        "\n",
        "# Define the ImageDataGenerator for preprocessing\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,  # Normalize pixel values to [0, 1]\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Define batch size and image size\n",
        "batch_size = 16  # Reduced the batch size to 16\n",
        "image_size = (256, 256)\n",
        "\n",
        "# Create a generator for training data\n",
        "train_generator = datagen.flow_from_directory(labeled_dataset_path,target_size=image_size,batch_size=batch_size,class_mode='categorical',shuffle=True)\n",
        "\n",
        "# Visualize a few preprocessed images\n",
        "sample_images, sample_labels = next(train_generator)\n",
        "\n",
        "plt.figure(figsize=(12, 9))\n",
        "for i in range(9):\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(sample_images[i])\n",
        "    plt.title(f\"Label: {np.argmax(sample_labels[i])}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "ZixcgRz0zDIq",
        "outputId": "ebe0a488-898a-45b2-d554-87598a42642c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 images belonging to 0 classes.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 0 is out of bounds for axis 0 with size 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-746e36f8aeb3>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Label: {np.argmax(sample_labels[i])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x900 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVAAAAD9CAYAAADqHS9dAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAXOElEQVR4nO3df1BVdf7H8RegXGwStGW5IHuN1das/IFBsGhO087dmMlh1z92YrUBlqlci5ryzm5KKmSWuG457CTlZLn2Ry60jjpNMJjLxjQVO04gM7WJjWHhNt2rbCvXxQLlfr5/NN6WAOV8hAv6fT5mzh/34+dzzvvt1Zfn3nM4RhljjAAAjkWPdQEAcKUiQAHAEgEKAJYIUACwRIACgCUCFAAsEaAAYIkABQBLBCgAWCJAAcCS4wB95513lJeXp2nTpikqKkr79++/5JrGxkbdeuutcrlcuuGGG7Rr1y6LUgFgfHEcoN3d3Zo/f76qqqqGNf/48eNasmSJ7rzzTrW2tuqxxx7T/fffrwMHDjguFgDGk6jLeZhIVFSU9u3bp6VLlw45Z/Xq1aqtrdVHH30UHvv1r3+t06dPq76+3vbQADDmJoz2AZqamuT1evuN5ebm6rHHHhtyTU9Pj3p6esKvQ6GQvvrqK/3gBz9QVFTUaJUK4CpljNGZM2c0bdo0RUeP3KWfUQ9Qv98vt9vdb8ztdisYDOrrr7/WpEmTBqypqKjQhg0bRrs0AP/PnDhxQj/60Y9GbH+jHqA2SktL5fP5wq+7uro0ffp0nThxQvHx8WNYGYArUTAYlMfj0eTJk0d0v6MeoMnJyQoEAv3GAoGA4uPjBz37lCSXyyWXyzVgPD4+ngAFYG2kvwIc9ftAc3Jy1NDQ0G/s4MGDysnJGe1DA8Cochyg//3vf9Xa2qrW1lZJ396m1Nraqo6ODknffvwuLCwMz1+5cqXa29v1+OOPq62tTS+88IJef/11rVq1amQ6AIAx4jhAP/jgAy1YsEALFiyQJPl8Pi1YsEBlZWWSpC+//DIcppL04x//WLW1tTp48KDmz5+v5557Ti+//LJyc3NHqAUAGBuXdR9opASDQSUkJKirq4vvQAE4NloZws/CA4AlAhQALBGgAGCJAAUASwQoAFgiQAHAEgEKAJYIUACwRIACgCUCFAAsEaAAYIkABQBLBCgAWCJAAcASAQoAlghQALBEgAKAJQIUACwRoABgiQAFAEsEKABYIkABwBIBCgCWCFAAsESAAoAlAhQALBGgAGCJAAUASwQoAFgiQAHAklWAVlVVKS0tTXFxccrOztahQ4cuOr+yslI33nijJk2aJI/Ho1WrVumbb76xKhgAxgvHAVpTUyOfz6fy8nK1tLRo/vz5ys3N1cmTJwedv3v3bq1Zs0bl5eU6cuSIXnnlFdXU1OiJJ5647OIBYCw5DtCtW7fqgQceUHFxsW6++WZt375d11xzjXbu3Dno/Pfff1+LFi3S8uXLlZaWprvuukvLli275FkrAIx3jgK0t7dXzc3N8nq93+0gOlper1dNTU2Drlm4cKGam5vDgdne3q66ujrdfffdQx6np6dHwWCw3wYA480EJ5M7OzvV19cnt9vdb9ztdqutrW3QNcuXL1dnZ6duv/12GWN0/vx5rVy58qIf4SsqKrRhwwYnpQFAxI36VfjGxkZt2rRJL7zwglpaWrR3717V1tZq48aNQ64pLS1VV1dXeDtx4sRolwkAjjk6A01MTFRMTIwCgUC/8UAgoOTk5EHXrF+/XgUFBbr//vslSXPnzlV3d7dWrFihtWvXKjp6YIa7XC65XC4npQFAxDk6A42NjVVGRoYaGhrCY6FQSA0NDcrJyRl0zdmzZweEZExMjCTJGOO0XgAYNxydgUqSz+dTUVGRMjMzlZWVpcrKSnV3d6u4uFiSVFhYqNTUVFVUVEiS8vLytHXrVi1YsEDZ2dk6duyY1q9fr7y8vHCQAsCVyHGA5ufn69SpUyorK5Pf71d6errq6+vDF5Y6Ojr6nXGuW7dOUVFRWrdunb744gv98Ic/VF5enp555pmR6wIAxkCUuQI+RweDQSUkJKirq0vx8fFjXQ6AK8xoZQg/Cw8AlghQALBEgAKAJQIUACwRoABgiQAFAEsEKABYIkABwBIBCgCWCFAAsESAAoAlAhQALBGgAGCJAAUASwQoAFgiQAHAEgEKAJYIUACwRIACgCUCFAAsEaAAYIkABQBLBCgAWCJAAcASAQoAlghQALBEgAKAJQIUACwRoABgiQAFAEtWAVpVVaW0tDTFxcUpOztbhw4duuj806dPq6SkRCkpKXK5XJo1a5bq6uqsCgaA8WKC0wU1NTXy+Xzavn27srOzVVlZqdzcXB09elRJSUkD5vf29urnP/+5kpKStGfPHqWmpurzzz/XlClTRqJ+ABgzUcYY42RBdna2brvtNm3btk2SFAqF5PF49Mgjj2jNmjUD5m/fvl1//OMf1dbWpokTJ1oVGQwGlZCQoK6uLsXHx1vtA8D/X6OVIY4+wvf29qq5uVler/e7HURHy+v1qqmpadA1b7zxhnJyclRSUiK32605c+Zo06ZN6uvrG/I4PT09CgaD/TYAGG8cBWhnZ6f6+vrkdrv7jbvdbvn9/kHXtLe3a8+ePerr61NdXZ3Wr1+v5557Tk8//fSQx6moqFBCQkJ483g8TsoEgIgY9avwoVBISUlJeumll5SRkaH8/HytXbtW27dvH3JNaWmpurq6wtuJEydGu0wAcMzRRaTExETFxMQoEAj0Gw8EAkpOTh50TUpKiiZOnKiYmJjw2E033SS/36/e3l7FxsYOWONyueRyuZyUBgAR5+gMNDY2VhkZGWpoaAiPhUIhNTQ0KCcnZ9A1ixYt0rFjxxQKhcJjn3zyiVJSUgYNTwC4Ujj+CO/z+bRjxw69+uqrOnLkiB588EF1d3eruLhYklRYWKjS0tLw/AcffFBfffWVHn30UX3yySeqra3Vpk2bVFJSMnJdAMAYcHwfaH5+vk6dOqWysjL5/X6lp6ervr4+fGGpo6ND0dHf5bLH49GBAwe0atUqzZs3T6mpqXr00Ue1evXqkesCAMaA4/tAxwL3gQK4HOPiPlAAwHcIUACwRIACgCUCFAAsEaAAYIkABQBLBCgAWCJAAcASAQoAlghQALBEgAKAJQIUACwRoABgiQAFAEsEKABYIkABwBIBCgCWCFAAsESAAoAlAhQALBGgAGCJAAUASwQoAFgiQAHAEgEKAJYIUACwRIACgCUCFAAsEaAAYMkqQKuqqpSWlqa4uDhlZ2fr0KFDw1pXXV2tqKgoLV261OawADCuOA7Qmpoa+Xw+lZeXq6WlRfPnz1dubq5Onjx50XWfffaZfve732nx4sXWxQLAeOI4QLdu3aoHHnhAxcXFuvnmm7V9+3Zdc8012rlz55Br+vr6dO+992rDhg2aMWPGZRUMAOOFowDt7e1Vc3OzvF7vdzuIjpbX61VTU9OQ65566iklJSXpvvvus68UAMaZCU4md3Z2qq+vT263u9+42+1WW1vboGveffddvfLKK2ptbR32cXp6etTT0xN+HQwGnZQJABExqlfhz5w5o4KCAu3YsUOJiYnDXldRUaGEhITw5vF4RrFKALDj6Aw0MTFRMTExCgQC/cYDgYCSk5MHzP/000/12WefKS8vLzwWCoW+PfCECTp69Khmzpw5YF1paal8Pl/4dTAYJEQBjDuOAjQ2NlYZGRlqaGgI34oUCoXU0NCghx9+eMD82bNn68MPP+w3tm7dOp05c0Z/+tOfhgxFl8sll8vlpDQAiDhHASpJPp9PRUVFyszMVFZWliorK9Xd3a3i4mJJUmFhoVJTU1VRUaG4uDjNmTOn3/opU6ZI0oBxALjSOA7Q/Px8nTp1SmVlZfL7/UpPT1d9fX34wlJHR4eio/kBJwBXvyhjjBnrIi4lGAwqISFBXV1dio+PH+tyAFxhRitDOFUEAEsEKABYIkABwBIBCgCWCFAAsESAAoAlAhQALBGgAGCJAAUASwQoAFgiQAHAEgEKAJYIUACwRIACgCUCFAAsEaAAYIkABQBLBCgAWCJAAcASAQoAlghQALBEgAKAJQIUACwRoABgiQAFAEsEKABYIkABwBIBCgCWCFAAsESAAoAlqwCtqqpSWlqa4uLilJ2drUOHDg05d8eOHVq8eLGmTp2qqVOnyuv1XnQ+AFwpHAdoTU2NfD6fysvL1dLSovnz5ys3N1cnT54cdH5jY6OWLVumt99+W01NTfJ4PLrrrrv0xRdfXHbxADCWoowxxsmC7Oxs3Xbbbdq2bZskKRQKyePx6JFHHtGaNWsuub6vr09Tp07Vtm3bVFhYOKxjBoNBJSQkqKurS/Hx8U7KBYBRyxBHZ6C9vb1qbm6W1+v9bgfR0fJ6vWpqahrWPs6ePatz587puuuuc1YpAIwzE5xM7uzsVF9fn9xud79xt9uttra2Ye1j9erVmjZtWr8Q/r6enh719PSEXweDQSdlAkBERPQq/ObNm1VdXa19+/YpLi5uyHkVFRVKSEgIbx6PJ4JVAsDwOArQxMRExcTEKBAI9BsPBAJKTk6+6Npnn31Wmzdv1ltvvaV58+ZddG5paam6urrC24kTJ5yUCQAR4ShAY2NjlZGRoYaGhvBYKBRSQ0ODcnJyhly3ZcsWbdy4UfX19crMzLzkcVwul+Lj4/ttADDeOPoOVJJ8Pp+KioqUmZmprKwsVVZWqru7W8XFxZKkwsJCpaamqqKiQpL0hz/8QWVlZdq9e7fS0tLk9/slSddee62uvfbaEWwFACLLcYDm5+fr1KlTKisrk9/vV3p6uurr68MXljo6OhQd/d2J7Ysvvqje3l796le/6ref8vJyPfnkk5dXPQCMIcf3gY4F7gMFcDnGxX2gAIDvEKAAYIkABQBLBCgAWCJAAcASAQoAlghQALBEgAKAJQIUACwRoABgiQAFAEsEKABYIkABwBIBCgCWCFAAsESAAoAlAhQALBGgAGCJAAUASwQoAFgiQAHAEgEKAJYIUACwRIACgCUCFAAsEaAAYIkABQBLBCgAWCJAAcASAQoAlqwCtKqqSmlpaYqLi1N2drYOHTp00fl//etfNXv2bMXFxWnu3Lmqq6uzKhYAxhPHAVpTUyOfz6fy8nK1tLRo/vz5ys3N1cmTJwed//7772vZsmW67777dPjwYS1dulRLly7VRx99dNnFA8BYijLGGCcLsrOzddttt2nbtm2SpFAoJI/Ho0ceeURr1qwZMD8/P1/d3d168803w2M//elPlZ6eru3btw/rmMFgUAkJCerq6lJ8fLyTcgFg1DJkgpPJvb29am5uVmlpaXgsOjpaXq9XTU1Ng65pamqSz+frN5abm6v9+/cPeZyenh719PSEX3d1dUn69jcBAJy6kB0OzxcvyVGAdnZ2qq+vT263u9+42+1WW1vboGv8fv+g8/1+/5DHqaio0IYNGwaMezweJ+UCQD///ve/lZCQMGL7cxSgkVJaWtrvrPX06dO6/vrr1dHRMaLNj7VgMCiPx6MTJ05cdV9N0NuV6WrtraurS9OnT9d11103ovt1FKCJiYmKiYlRIBDoNx4IBJScnDzomuTkZEfzJcnlcsnlcg0YT0hIuKre1Avi4+Ovyr4kertSXa29RUeP7J2bjvYWGxurjIwMNTQ0hMdCoZAaGhqUk5Mz6JqcnJx+8yXp4MGDQ84HgCuF44/wPp9PRUVFyszMVFZWliorK9Xd3a3i4mJJUmFhoVJTU1VRUSFJevTRR3XHHXfoueee05IlS1RdXa0PPvhAL7300sh2AgAR5jhA8/PzderUKZWVlcnv9ys9PV319fXhC0UdHR39TpMXLlyo3bt3a926dXriiSf0k5/8RPv379ecOXOGfUyXy6Xy8vJBP9Zfya7WviR6u1Jdrb2NVl+O7wMFAHyLn4UHAEsEKABYIkABwBIBCgCWxk2AXq2PyHPS144dO7R48WJNnTpVU6dOldfrveTvw1hy+p5dUF1draioKC1dunR0C7wMTns7ffq0SkpKlJKSIpfLpVmzZo3LP5NO+6qsrNSNN96oSZMmyePxaNWqVfrmm28iVO3wvfPOO8rLy9O0adMUFRV10WdtXNDY2Khbb71VLpdLN9xwg3bt2uX8wGYcqK6uNrGxsWbnzp3mn//8p3nggQfMlClTTCAQGHT+e++9Z2JiYsyWLVvMxx9/bNatW2cmTpxoPvzwwwhXfnFO+1q+fLmpqqoyhw8fNkeOHDG/+c1vTEJCgvnXv/4V4covzWlvFxw/ftykpqaaxYsXm1/+8peRKdYhp7319PSYzMxMc/fdd5t3333XHD9+3DQ2NprW1tYIV35xTvt67bXXjMvlMq+99po5fvy4OXDggElJSTGrVq2KcOWXVldXZ9auXWv27t1rJJl9+/ZddH57e7u55pprjM/nMx9//LF5/vnnTUxMjKmvr3d03HERoFlZWaakpCT8uq+vz0ybNs1UVFQMOv+ee+4xS5Ys6TeWnZ1tfvvb345qnU457ev7zp8/byZPnmxeffXV0SrRmk1v58+fNwsXLjQvv/yyKSoqGrcB6rS3F1980cyYMcP09vZGqkQrTvsqKSkxP/vZz/qN+Xw+s2jRolGt83INJ0Aff/xxc8stt/Qby8/PN7m5uY6ONeYf4S88Is/r9YbHhvOIvP+dL337iLyh5o8Fm76+7+zZszp37tyIPwDhctn29tRTTykpKUn33XdfJMq0YtPbG2+8oZycHJWUlMjtdmvOnDnatGmT+vr6IlX2Jdn0tXDhQjU3N4c/5re3t6uurk533313RGoeTSOVIWP+NKZIPSIv0mz6+r7Vq1dr2rRpA97osWbT27vvvqtXXnlFra2tEajQnk1v7e3t+vvf/657771XdXV1OnbsmB566CGdO3dO5eXlkSj7kmz6Wr58uTo7O3X77bfLGKPz589r5cqVeuKJJyJR8qgaKkOCwaC+/vprTZo0aVj7GfMzUAxu8+bNqq6u1r59+xQXFzfW5VyWM2fOqKCgQDt27FBiYuJYlzPiQqGQkpKS9NJLLykjI0P5+flau3btsP/HhfGqsbFRmzZt0gsvvKCWlhbt3btXtbW12rhx41iXNm6M+RlopB6RF2k2fV3w7LPPavPmzfrb3/6mefPmjWaZVpz29umnn+qzzz5TXl5eeCwUCkmSJkyYoKNHj2rmzJmjW/Qw2bxvKSkpmjhxomJiYsJjN910k/x+v3p7exUbGzuqNQ+HTV/r169XQUGB7r//fknS3Llz1d3drRUrVmjt2rUj/mi4SBoqQ+Lj44d99imNgzPQq/UReTZ9SdKWLVu0ceNG1dfXKzMzMxKlOua0t9mzZ+vDDz9Ua2trePvFL36hO++8U62trePqfxqwed8WLVqkY8eOhf9RkKRPPvlEKSkp4yI8Jbu+zp49OyAkL/wjYa7wR2iMWIY4u741Oqqrq43L5TK7du0yH3/8sVmxYoWZMmWK8fv9xhhjCgoKzJo1a8Lz33vvPTNhwgTz7LPPmiNHjpjy8vJxexuTk742b95sYmNjzZ49e8yXX34Z3s6cOTNWLQzJaW/fN56vwjvtraOjw0yePNk8/PDD5ujRo+bNN980SUlJ5umnnx6rFgbltK/y8nIzefJk85e//MW0t7ebt956y8ycOdPcc889Y9XCkM6cOWMOHz5sDh8+bCSZrVu3msOHD5vPP//cGGPMmjVrTEFBQXj+hduYfv/735sjR46YqqqqK/c2JmOMef7558306dNNbGysycrKMv/4xz/Cv3bHHXeYoqKifvNff/11M2vWLBMbG2tuueUWU1tbG+GKh8dJX9dff72RNGArLy+PfOHD4PQ9+1/jOUCNcd7b+++/b7Kzs43L5TIzZswwzzzzjDl//nyEq740J32dO3fOPPnkk2bmzJkmLi7OeDwe89BDD5n//Oc/kS/8Et5+++1B/+5c6KeoqMjccccdA9akp6eb2NhYM2PGDPPnP//Z8XF5nB0AWBrz70AB4EpFgAKAJQIUACwRoABgiQAFAEsEKABYIkABwBIBCgCWCFAAsESAAoAlAhQALBGgAGDp/wBM1MvfHXZxoQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Path to the labeled dataset\n",
        "labeled_dataset_path = Path(\"/content/labeled_dataset\")\n",
        "\n",
        "# Define the ImageDataGenerator for preprocessing\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,  # Normalize pixel values to [0, 1]\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Define batch size and image size\n",
        "batch_size = 32\n",
        "image_size = (256, 256)\n",
        "\n",
        "# Create a generator for training data\n",
        "train_generator = datagen.flow_from_directory(labeled_dataset_path,target_size=image_size,batch_size=batch_size,class_mode='categorical',shuffle=True)\n",
        "\n",
        "# Print the length of the train_generator\n",
        "print(f\"Length of train_generator: {len(train_generator)}\")\n",
        "\n",
        "# Print the batch_size and target_size\n",
        "print(f\"Batch size: {batch_size}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQLZ9ECPGYdr",
        "outputId": "606cdc8a-4b9d-40aa-fdef-8fe2a3b7a85a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 images belonging to 0 classes.\n",
            "Length of train_generator: 0\n",
            "Batch size: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGi1o2tF-WTW",
        "outputId": "3c894201-7135-4dbc-fff2-432e3d0cd309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create a basic interface that allows users to input room dimensions, upload images, select style preferences, specify budget constraints, and interact with design suggestions."
      ],
      "metadata": {
        "id": "LxUI1GlpAgkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xauth\n",
        "!xauth add :0 . trusted\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiFiqEgXHCN3",
        "outputId": "096954d4-1556-45f1-f302-591849338f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "xauth is already the newest version (1:1.1-1build2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "xauth:  file /root/.Xauthority does not exist\n",
            "xauth: (argv):1:  key contains odd number of or non-hex characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tkinter\n",
        "import tkinter as tk\n",
        "from tkinter import filedialog\n",
        "from tkinter import messagebox\n",
        "from PIL import Image, ImageTk\n",
        "\n",
        "# Function to handle image upload\n",
        "def upload_image():\n",
        "    global image_path\n",
        "    image_path = filedialog.askopenfilename()\n",
        "    if image_path:\n",
        "        image = Image.open(image_path)\n",
        "        image = image.resize((250, 250), Image.ANTIALIAS)\n",
        "        image = ImageTk.PhotoImage(image)\n",
        "        image_label.config(image=image)\n",
        "        image_label.image = image\n",
        "    else:\n",
        "        messagebox.showerror(\"Error\", \"No image selected.\")\n",
        "\n",
        "# Function to handle design suggestions\n",
        "def get_design_suggestions():\n",
        "    # Placeholder function for AI design suggestions\n",
        "    messagebox.showinfo(\"Design Suggestions\", \"Here are some design suggestions based on your preferences.\")\n",
        "\n",
        "# Create the main window\n",
        "window = tk.Tk()\n",
        "window.title(\"AI Interior Design Assistant\")\n",
        "\n",
        "# Room Dimensions\n",
        "room_label = tk.Label(window, text=\"Enter Room Dimensions:\")\n",
        "room_label.pack()\n",
        "\n",
        "length_label = tk.Label(window, text=\"Length (ft):\")\n",
        "length_label.pack()\n",
        "length_entry = tk.Entry(window)\n",
        "length_entry.pack()\n",
        "\n",
        "width_label = tk.Label(window, text=\"Width (ft):\")\n",
        "width_label.pack()\n",
        "width_entry = tk.Entry(window)\n",
        "width_entry.pack()\n",
        "\n",
        "# Image Upload\n",
        "image_label = tk.Label(window, text=\"Upload Room Image:\")\n",
        "image_label.pack()\n",
        "\n",
        "upload_button = tk.Button(window, text=\"Upload Image\", command=upload_image)\n",
        "upload_button.pack()\n",
        "\n",
        "# Style Preferences\n",
        "style_label = tk.Label(window, text=\"Select Style Preference:\")\n",
        "style_label.pack()\n",
        "\n",
        "style_options = [\"Modern\", \"Vintage\", \"Minimalist\", \"Rustic\"]\n",
        "style_var = tk.StringVar(window)\n",
        "style_var.set(style_options[0])  # Default option\n",
        "\n",
        "style_menu = tk.OptionMenu(window, style_var, *style_options)\n",
        "style_menu.pack()\n",
        "\n",
        "# Budget Constraints\n",
        "budget_label = tk.Label(window, text=\"Specify Budget ($):\")\n",
        "budget_label.pack()\n",
        "\n",
        "budget_entry = tk.Entry(window)\n",
        "budget_entry.pack()\n",
        "\n",
        "# Button to Get Design Suggestions\n",
        "suggestions_button = tk.Button(window, text=\"Get Design Suggestions\", command=get_design_suggestions)\n",
        "suggestions_button.pack()\n",
        "\n",
        "# Run the Tkinter event loop\n",
        "window.mainloop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "UjvuG_RU-aT0",
        "outputId": "7c8b02f8-94d4-45f0-d61d-a11383e1da97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tkinter (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tkinter\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "no display name and no $DISPLAY environment variable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-9ec91259d7c4>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Create the main window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AI Interior Design Assistant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2297\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2298\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2299\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2301\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "install TensorFlow and Keras using pip"
      ],
      "metadata": {
        "id": "lNfizOPk_pYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow keras\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M-xrPWr_eU4",
        "outputId": "18f5b303-432b-45c2-f14a-4067815f2dc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.7)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "define and train a basic CNN model"
      ],
      "metadata": {
        "id": "gMkLJnSa_tNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Define the CNN model\n",
        "def create_cnn_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')  # Adjust num_classes based on your dataset\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Define input shape and number of classes\n",
        "input_shape = (256, 256, 3)  # Assuming images are resized to 256x256 pixels\n",
        "num_classes = 4  # For example: Modern, Vintage, Minimalist, Rustic\n",
        "\n",
        "# Create an instance of the CNN model\n",
        "model = create_cnn_model(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfbRXNF8_wzG",
        "outputId": "67c68483-b454-45d1-df2f-b6f272f87345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 127, 127, 32)      0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 62, 62, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 60, 60, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 30, 30, 128)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 115200)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               29491456  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 1028      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 29585732 (112.86 MB)\n",
            "Trainable params: 29585732 (112.86 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " prepare the dataset.Use the labeled dataset created earlier"
      ],
      "metadata": {
        "id": "7KTZqeBp_zbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the labeled dataset\n",
        "labeled_dataset_path = \"/content/labeled_dataset\"\n",
        "\n",
        "# Create ImageDataGenerator for preprocessing and loading data\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,  # Normalize pixel values to [0, 1]\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Create the training and validation data generators\n",
        "batch_size = 32\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    labeled_dataset_path,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    labeled_dataset_path,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WsTqBj4_936",
        "outputId": "e99ff99c-b0b2-4165-942f-749aa3eef1cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 images belonging to 0 classes.\n",
            "Found 0 images belonging to 0 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train the CNN model"
      ],
      "metadata": {
        "id": "mXfc4OZoAAz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "epochs = 10\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples//batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples//batch_size\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "W-E2WBRDAEGV",
        "outputId": "7699c4bd-8462-4693-d6ab-1908023e6e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unexpected value for `steps_per_epoch`. Received value is 0. Please check the docstring for `model.fit()` for supported values.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-06e2275fc3b0>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1276\u001b[0m                 \u001b[0;34m\"Unexpected value for `steps_per_epoch`. Received value is 0. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m                 \u001b[0;34m\"Please check the docstring for `model.fit()` for supported \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unexpected value for `steps_per_epoch`. Received value is 0. Please check the docstring for `model.fit()` for supported values."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model is trained, use it to predict the style of uploaded images"
      ],
      "metadata": {
        "id": "im45T_BSAGRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_style(image_path):\n",
        "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(256, 256))\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
        "    img_array /= 255.\n",
        "\n",
        "    prediction = model.predict(img_array)\n",
        "    predicted_class = tf.argmax(prediction, axis=1).numpy()[0]\n",
        "\n",
        "    style_mapping = {\n",
        "        0: \"Modern\",\n",
        "        1: \"Vintage\",\n",
        "        2: \"Minimalist\",\n",
        "        3: \"Rustic\"\n",
        "    }\n",
        "\n",
        "    predicted_style = style_mapping[predicted_class]\n",
        "    return predicted_style\n",
        "\n",
        "# Example usage\n",
        "image_path = \"path_to_your_uploaded_image.jpg\"\n",
        "predicted_style = predict_style(image_path)\n",
        "print(\"Predicted Style:\", predicted_style)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "OxKvyCnWAM6g",
        "outputId": "b51f1a33-ddae-4305-a8a1-696f88fd7870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'path_to_your_uploaded_image.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-9322578e1f96>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"path_to_your_uploaded_image.jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mpredicted_style\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted Style:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_style\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-9322578e1f96>\u001b[0m in \u001b[0;36mpredict_style\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create batch axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimg_array\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/image_utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_your_uploaded_image.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generator Network:**\n",
        "The generator takes random noise as input and generates new design layouts.\n",
        "\n",
        "**Discriminator Network:**\n",
        "The discriminator evaluates whether an input design is real (from the dataset) or fake (generated by the generator).\n",
        "\n",
        "**GAN Model:**\n",
        "The GAN combines the generator and discriminator into a single model. The goal is to train the generator to fool the discriminator into classifying its generated designs as real."
      ],
      "metadata": {
        "id": "swLn3qHTAO9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the Generator Network\n",
        "def build_generator(latent_dim):\n",
        "    input_layer = Input(shape=(latent_dim,))\n",
        "    x = Dense(128 * 16 * 16, activation='relu')(input_layer)\n",
        "    x = Reshape((16, 16, 128))(x)\n",
        "    x = Conv2DTranspose(128, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "    x = Conv2DTranspose(64, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "    x = Conv2DTranspose(32, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "    output_layer = Conv2DTranspose(3, kernel_size=3, strides=1, padding='same', activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(input_layer, output_layer)\n",
        "    return model\n",
        "\n",
        "# Define the Discriminator Network\n",
        "def build_discriminator(input_shape):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    x = Conv2D(32, kernel_size=3, strides=2, padding='same', activation='relu')(input_layer)\n",
        "    x = Conv2D(64, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "    x = Conv2D(128, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "    x = Conv2D(256, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "    x = Flatten()(x)\n",
        "    output_layer = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(input_layer, output_layer)\n",
        "    return model\n",
        "\n",
        "# Define the GAN model\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    gan_input = Input(shape=(latent_dim,))\n",
        "    gan_output = discriminator(generator(gan_input))\n",
        "    model = Model(gan_input, gan_output)\n",
        "    return model\n",
        "\n",
        "# Define constants\n",
        "latent_dim = 100  # Dimension of the latent space\n",
        "input_shape = (256, 256, 3)  # Image dimensions\n",
        "\n",
        "# Build and compile the models\n",
        "generator = build_generator(latent_dim)\n",
        "discriminator = build_discriminator(input_shape)\n",
        "\n",
        "generator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
        "\n",
        "gan = build_gan(generator, discriminator)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
        "\n",
        "# Print model summaries\n",
        "generator.summary()\n",
        "discriminator.summary()\n",
        "gan.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "E_ePFSAWBxa6",
        "outputId": "37148b28-1ba3-422f-f290-18ff959f9e9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling layer \"model_1\" (type Functional).\n\nInput 0 of layer \"dense_3\" is incompatible with the layer: expected axis -1 of input shape to have value 65536, but received input with shape (None, 16384)\n\nCall arguments received by layer \"model_1\" (type Functional):\n  • inputs=tf.Tensor(shape=(None, 128, 128, 3), dtype=float32)\n  • training=None\n  • mask=None",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e558f3e34cb6>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0002\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0002\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-e558f3e34cb6>\u001b[0m in \u001b[0;36mbuild_gan\u001b[0;34m(generator, discriminator)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mgan_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mgan_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgan_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgan_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    278\u001b[0m                     \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 }:\n\u001b[0;32m--> 280\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    281\u001b[0m                         \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                         \u001b[0;34mf\"incompatible with the layer: expected axis {axis} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"model_1\" (type Functional).\n\nInput 0 of layer \"dense_3\" is incompatible with the layer: expected axis -1 of input shape to have value 65536, but received input with shape (None, 16384)\n\nCall arguments received by layer \"model_1\" (type Functional):\n  • inputs=tf.Tensor(shape=(None, 128, 128, 3), dtype=float32)\n  • training=None\n  • mask=None"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "define functions for training the GAN"
      ],
      "metadata": {
        "id": "KSSBoVOZB45a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan(generator, discriminator, gan, dataset, latent_dim, epochs, batch_size):\n",
        "    for epoch in range(epochs):\n",
        "        for batch in dataset:\n",
        "            # Train Discriminator\n",
        "            noise = tf.random.normal([batch_size, latent_dim])\n",
        "            fake_images = generator.predict(noise)\n",
        "            real_images = batch\n",
        "\n",
        "            combined_images = np.concatenate([real_images, fake_images])\n",
        "            labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
        "            labels += 0.05 * np.random.random(labels.shape)\n",
        "\n",
        "            d_loss = discriminator.train_on_batch(combined_images, labels)\n",
        "\n",
        "            # Train Generator\n",
        "            noise = tf.random.normal([batch_size, latent_dim])\n",
        "            misleading_labels = np.ones((batch_size, 1))\n",
        "\n",
        "            g_loss = gan.train_on_batch(noise, misleading_labels)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Discriminator Loss: {d_loss}, Generator Loss: {g_loss}\")\n",
        "\n",
        "# Train the GAN\n",
        "epochs = 50\n",
        "batch_size = 32\n",
        "\n",
        "# Path to your labeled dataset\n",
        "labeled_dataset_path = \"path_to_your_labeled_dataset\"\n",
        "\n",
        "# Create ImageDataGenerator for loading data\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "dataset = datagen.flow_from_directory(\n",
        "    labeled_dataset_path,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=None\n",
        ")\n",
        "\n",
        "# Train the GAN\n",
        "train_gan(generator, discriminator, gan, dataset, latent_dim, epochs, batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "e-qz94xFB9Yb",
        "outputId": "8ceffdc1-f8f0-4ab7-8970-06c65f5f1996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'path_to_your_labeled_dataset'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-9966e8bf9be0>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Create ImageDataGenerator for loading data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mdatagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m dataset = datagen.flow_from_directory(\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mlabeled_dataset_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m         \"\"\"\n\u001b[0;32m-> 1649\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1650\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_your_labeled_dataset'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "use the trained generator to generate new design layouts"
      ],
      "metadata": {
        "id": "vY9KuPiHB_8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_designs(generator, latent_dim, num_samples):\n",
        "    noise = np.random.normal(0, 1, (num_samples, latent_dim))\n",
        "    generated_designs = generator.predict(noise)\n",
        "    return generated_designs\n",
        "\n",
        "# Generate new designs\n",
        "num_samples = 5\n",
        "generated_designs = generate_designs(generator, latent_dim, num_samples)\n",
        "\n",
        "# Visualize the generated designs\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(num_samples):\n",
        "    plt.subplot(1, num_samples, i + 1)\n",
        "    plt.imshow(generated_designs[i])\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O8ST3rRPCDpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0sLknmUxCFvK"
      }
    }
  ]
}